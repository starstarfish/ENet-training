{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxejMOxJjr_a"
      },
      "source": [
        "# Sleep staging on the Sleep Physionet dataset\n",
        "\n",
        "In this tutorial, we will learn how to train a convolutional neural network on raw EEG data to classify sleep stages.\n",
        "\n",
        "This tutorial is based on the [MNE-Python](https://mne.tools/stable/auto_tutorials/sample-datasets/plot_sleep.html) and [braindecode](https://braindecode.org/auto_examples/plot_sleep_staging.html) sleep staging examples, the [`mne-torch`](https://github.com/mne-tools/mne-torch) repository, as well as\n",
        "\n",
        "> Chambon, S., Galtier, M. N., Arnal, P. J., Wainrib, G., & Gramfort, A. (2018). A deep learning architecture for temporal sleep stage classification using multivariate and multimodal time series. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 26(4), 758-769.\n",
        "\n",
        "## Sleep staging\n",
        "\n",
        "Sleep staging is the process of identifying the sleep stage someone is in by analyzing their EEG and other physiological signals. Sleep recordings are traditionally divided into 30-s windows, and one of five categories (\"stages\") is attributed to each window:\n",
        "\n",
        "1. W: wakefulness\n",
        "2. N1: light sleep\n",
        "3. N2: deeper sleep\n",
        "4. N3: deep sleep\n",
        "5. R: rapid eye movement\n",
        "\n",
        "Sleep staging usually relies on capturing changes in the spectral properties of the EEG as well as transient events (e.g., sleep spindles, k-complexes, slow waves, etc.) that occur under the different sleep stages.\n",
        "\n",
        "In this tutorial, we will train a convolutional neural network (ConvNet) to perform sleep staging on unseen raw EEG. We will use the [Sleep Physionet](https://physionet.org/content/sleep-edfx/1.0.0/) dataset, which contains 153 overnight sleep recordings from 78 individuals. These recordings were manually staged by sleep experts, providing us with the required classification targets to train and evaluate our ConvNet on.\n",
        "\n",
        "## Objective of the tutorial\n",
        "\n",
        "This tutorial is meant to be a general hands-on introduction to training neural networks on EEG data. With this in mind, the default training hyperparameters below are set such that computations don't take too long.\n",
        "\n",
        "Once you have been through the whole material though, you are encouraged to experiment with the different elements of the pipeline (e.g., data preprocessing, neural network architecture, optimization parameters, etc.) to try to improve performance as much as possible.\n",
        "\n",
        "## Steps\n",
        "\n",
        "This notebook is divided into the following sections:\n",
        "\n",
        "0. [Set up environment](#0.-Setting-up-the-environment)\n",
        "1. [Load data](#1.-Loading-data)\n",
        "2. [Preprocess data (filter, window)](#2.-Preprocessing-raw-data)\n",
        "3. [Make splits](#3.-Making-train,-valid-and-test-splits)\n",
        "4. [Create model](#4.-Creating-the-neural-network)\n",
        "5. [Train and monitor](#5.-Train-and-monitor-network)\n",
        "6. [Visualize results](#6.-Visualizing-results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GyW1fUpjr_f"
      },
      "source": [
        "## 0. Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pSamkH5jr_f",
        "outputId": "e9a61a3a-fca5-4580-daed-9fd0a38f7cc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch  # should already be installed on colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VB3nDSm_jr_h",
        "outputId": "a232e6d7-69e3-4fdb-c6bc-d32454df3c8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU found. Training will be carried out on CPU, which might be slower.\n",
            "\n",
            "If running on Google Colab, you can request a GPU runtime by clicking\n",
            "`Runtime/Change runtime type` in the top bar menu, then selecting 'GPU'\n",
            "under 'Hardware accelerator'.\n"
          ]
        }
      ],
      "source": [
        "# Identify whether a CUDA-enabled GPU is available\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print('CUDA-enabled GPU found. Training should be faster.')\n",
        "else:\n",
        "    print('No GPU found. Training will be carried out on CPU, which might be '\n",
        "          'slower.\\n\\nIf running on Google Colab, you can request a GPU runtime by'\n",
        "          ' clicking\\n`Runtime/Change runtime type` in the top bar menu, then '\n",
        "          'selecting \\'GPU\\'\\nunder \\'Hardware accelerator\\'.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmSRS9iqjr_h",
        "outputId": "0a64212d-d54b-4d0f-93b1-8836da929755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mne\n",
            "  Downloading mne-1.9.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mne) (3.1.5)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.11/dist-packages (from mne) (0.4)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from mne) (3.10.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from mne) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mne) (24.2)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.11/dist-packages (from mne) (1.8.2)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from mne) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mne) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (2.32.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mne) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->mne) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2025.1.31)\n",
            "Downloading mne-1.9.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mne\n",
            "Successfully installed mne-1.9.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages for colab\n",
        "!pip install mne\n",
        "!pip install torch\n",
        "!pip install matplotlib\n",
        "!pip install scikit-learn\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_cJeVBLxjr_i"
      },
      "outputs": [],
      "source": [
        "# import general modules\n",
        "import os\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "e7vycdcDjr_i"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XAAaq2mjr_i"
      },
      "source": [
        "## 1. Loading data\n",
        "\n",
        "We start by loading the raw EEG recordings from the Sleep Physionet dataset. MNE-Python already contains a function `fetch_data` which downloads the recordings locally. We then need to read each file from the disk.\n",
        "\n",
        "To make the first pass through this tutorial faster, we only load a part of the entire Sleep Physionet dataset (30 recordings out of 153). Once you are able to run the whole tutorial and are ready to work on improving the performance of the model, you can try loading more subjects and recordings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4dNwtIMRjr_j"
      },
      "outputs": [],
      "source": [
        "import mne\n",
        "from mne.datasets.sleep_physionet.age import fetch_data\n",
        "\n",
        "mne.set_log_level('ERROR')  # To avoid flooding the cell outputs with messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Jhsax7oRjr_j"
      },
      "outputs": [],
      "source": [
        "subjects = range(30)\n",
        "recordings = [1]\n",
        "\n",
        "# To load all subjects and recordings, uncomment the next line\n",
        "# subjects, recordings = range(83), [1, 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKc-f-7djr_j",
        "outputId": "181476cd-cc69-49fb-9702-c14bab195539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading data from 'https://physionet.org/physiobank/database/sleep-edfx/sleep-cassette//SC4001E0-PSG.edf' to file '/root/mne_data/physionet-sleep-data/SC4001E0-PSG.edf'.\n",
            "Downloading data from 'https://physionet.org/physiobank/database/sleep-edfx/sleep-cassette//SC4001EC-Hypnogram.edf' to file '/root/mne_data/physionet-sleep-data/SC4001EC-Hypnogram.edf'.\n",
            "Downloading data from 'https://physionet.org/physiobank/database/sleep-edfx/sleep-cassette//SC4011E0-PSG.edf' to file '/root/mne_data/physionet-sleep-data/SC4011E0-PSG.edf'.\n",
            "Downloading data from 'https://physionet.org/physiobank/database/sleep-edfx/sleep-cassette//SC4011EH-Hypnogram.edf' to file '/root/mne_data/physionet-sleep-data/SC4011EH-Hypnogram.edf'.\n",
            "Downloading data from 'https://physionet.org/physiobank/database/sleep-edfx/sleep-cassette//SC4021E0-PSG.edf' to file '/root/mne_data/physionet-sleep-data/SC4021E0-PSG.edf'.\n",
            "Downloading data from 'https://physionet.org/physiobank/database/sleep-edfx/sleep-cassette//SC4021EH-Hypnogram.edf' to file '/root/mne_data/physionet-sleep-data/SC4021EH-Hypnogram.edf'.\n",
            "Downloading data from 'https://physionet.org/physiobank/database/sleep-edfx/sleep-cassette//SC4031E0-PSG.edf' to file '/root/mne_data/physionet-sleep-data/SC4031E0-PSG.edf'.\n",
            "Downloading data from 'https://physionet.org/physiobank/database/sleep-edfx/sleep-cassette//SC4031EC-Hypnogram.edf' to file '/root/mne_data/physionet-sleep-data/SC4031EC-Hypnogram.edf'.\n",
            "Downloading data from 'https://physionet.org/physiobank/database/sleep-edfx/sleep-cassette//SC4041E0-PSG.edf' to file '/root/mne_data/physionet-sleep-data/SC4041E0-PSG.edf'.\n",
            "Downloading data from 'https://physionet.org/physiobank/database/sleep-edfx/sleep-cassette//SC4041EC-Hypnogram.edf' to file '/root/mne_data/physionet-sleep-data/SC4041EC-Hypnogram.edf'.\n",
            "Downloading data from 'https://physionet.org/physiobank/database/sleep-edfx/sleep-cassette//SC4051E0-PSG.edf' to file '/root/mne_data/physionet-sleep-data/SC4051E0-PSG.edf'.\n",
            "Downloading data from 'https://physionet.org/physiobank/database/sleep-edfx/sleep-cassette//SC4051EC-Hypnogram.edf' to file '/root/mne_data/physionet-sleep-data/SC4051EC-Hypnogram.edf'.\n",
            "Downloading data from 'https://physionet.org/physiobank/database/sleep-edfx/sleep-cassette//SC4061E0-PSG.edf' to file '/root/mne_data/physionet-sleep-data/SC4061E0-PSG.edf'.\n",
            "Downloading data from 'https://physionet.org/physiobank/database/sleep-edfx/sleep-cassette//SC4061EC-Hypnogram.edf' to file '/root/mne_data/physionet-sleep-data/SC4061EC-Hypnogram.edf'.\n",
            "Downloading data from 'https://physionet.org/physiobank/database/sleep-edfx/sleep-cassette//SC4071E0-PSG.edf' to file '/root/mne_data/physionet-sleep-data/SC4071E0-PSG.edf'.\n"
          ]
        }
      ],
      "source": [
        "fnames = fetch_data(subjects=subjects, recording=recordings, on_missing='warn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Hp1Pdt3jr_j"
      },
      "outputs": [],
      "source": [
        "def load_sleep_physionet_raw(raw_fname, annot_fname, load_eeg_only=True,\n",
        "                             crop_wake_mins=30):\n",
        "    \"\"\"Load a recording from the Sleep Physionet dataset.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    raw_fname : str\n",
        "        Path to the .edf file containing the raw data.\n",
        "    annot_fname : str\n",
        "        Path to the annotation file.\n",
        "    load_eeg_only : bool\n",
        "        If True, only keep EEG channels and discard other modalities\n",
        "        (speeds up loading).\n",
        "    crop_wake_mins : float\n",
        "        Number of minutes of wake events before and after sleep events.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    mne.io.Raw :\n",
        "        Raw object containing the EEG and annotations.\n",
        "    \"\"\"\n",
        "    mapping = {'EOG horizontal': 'eog',\n",
        "               'Resp oro-nasal': 'misc',\n",
        "               'EMG submental': 'misc',\n",
        "               'Temp rectal': 'misc',\n",
        "               'Event marker': 'misc'}\n",
        "    exclude = mapping.keys() if load_eeg_only else ()\n",
        "\n",
        "    raw = mne.io.read_raw_edf(raw_fname, exclude=exclude)\n",
        "    annots = mne.read_annotations(annot_fname)\n",
        "    raw.set_annotations(annots, emit_warning=False)\n",
        "    if not load_eeg_only:\n",
        "        raw.set_channel_types(mapping)\n",
        "\n",
        "    if crop_wake_mins > 0:  # Cut start and end Wake periods\n",
        "        # Find first and last sleep stages\n",
        "        mask = [x[-1] in ['1', '2', '3', '4', 'R']\n",
        "                for x in annots.description]\n",
        "        sleep_event_inds = np.where(mask)[0]\n",
        "\n",
        "        # Crop raw\n",
        "        tmin = annots[int(sleep_event_inds[0])]['onset'] - \\\n",
        "               crop_wake_mins * 60\n",
        "        tmax = annots[int(sleep_event_inds[-1])]['onset'] + \\\n",
        "               crop_wake_mins * 60\n",
        "        raw.crop(tmin=tmin, tmax=tmax)\n",
        "\n",
        "    # Rename EEG channels\n",
        "    ch_names = {i: i.replace('EEG ', '')\n",
        "                for i in raw.ch_names if 'EEG' in i}\n",
        "    mne.rename_channels(raw.info, ch_names)\n",
        "\n",
        "    # Save subject and recording information in raw.info\n",
        "    basename = os.path.basename(raw_fname)\n",
        "    subj_nb, rec_nb = int(basename[3:5]), int(basename[5])\n",
        "    # Instead of direct assignment, use update()\n",
        "    #raw.info['subject_info'] = {'id': subj_nb, 'rec_id': rec_nb}\n",
        "    raw.info['subject_info'].update({'id': subj_nb, 'rec_id': rec_nb})\n",
        "\n",
        "    return raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUDBW98ojr_k"
      },
      "outputs": [],
      "source": [
        "# Load recordings\n",
        "raws = [load_sleep_physionet_raw(f[0], f[1]) for f in fnames]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km-DKGPsjr_k"
      },
      "outputs": [],
      "source": [
        "# Plot a recording as a sanity check\n",
        "raws[0].plot();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O334qd2Ijr_k"
      },
      "source": [
        "## 2. Preprocessing raw data\n",
        "\n",
        "Next, we need to preprocess the raw data. Here, we use a simple filtering step, followed by the extraction of 30-s windows.\n",
        "\n",
        "Sleep EEG data has most of its relevant information below 30 Hz. Therefore, to mitigate the impact of higher frequency noise, we apply a lowpass filter with cutoff frequency of 30 Hz to our recordings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oD7h027Gjr_k"
      },
      "outputs": [],
      "source": [
        "l_freq, h_freq = None, 30\n",
        "\n",
        "for raw in raws:\n",
        "    raw.load_data().filter(l_freq, h_freq)  # filtering happens in-place"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNgv4OhKjr_k"
      },
      "outputs": [],
      "source": [
        "# Plot the power spectrum of a recording as sanity check\n",
        "raws[0].plot_psd();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-o86md6jr_k"
      },
      "source": [
        "From the power spectral density plot, we can see that our filter has indeed cut off most of the power above 30 Hz.\n",
        "\n",
        "Before proceeding to extracting 30-s windows (also called *epochs*) from the filtered data, we define a few functions that we will need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9uxoc9_jr_l"
      },
      "outputs": [],
      "source": [
        "def extract_epochs(raw, chunk_duration=30.):\n",
        "    \"\"\"Extract non-overlapping epochs from raw data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    raw : mne.io.Raw\n",
        "        Raw data object to be windowed.\n",
        "    chunk_duration : float\n",
        "        Length of a window.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        Epoched data, of shape (n_epochs, n_channels, n_times).\n",
        "    np.ndarray\n",
        "        Event identifiers for each epoch, shape (n_epochs,).\n",
        "    \"\"\"\n",
        "    # map sleep stage description to event_id\n",
        "    annotation_desc_2_event_id = {\n",
        "        'Sleep stage W': 1,\n",
        "        'Sleep stage 1': 2,\n",
        "        'Sleep stage 2': 3,\n",
        "        'Sleep stage 3': 4,\n",
        "        'Sleep stage 4': 4,\n",
        "        'Sleep stage R': 5}\n",
        "\n",
        "    events, _ = mne.events_from_annotations(\n",
        "        raw, event_id=annotation_desc_2_event_id,\n",
        "        chunk_duration=chunk_duration)\n",
        "\n",
        "    # create a new event_id that unifies stages 3 and 4\n",
        "    event_id = {\n",
        "        'Sleep stage W': 1,\n",
        "        'Sleep stage 1': 2,\n",
        "        'Sleep stage 2': 3,\n",
        "        'Sleep stage 3/4': 4,\n",
        "        'Sleep stage R': 5}\n",
        "\n",
        "    # tmax in included, each epoch = 30s, sfreq = sampling frequency\n",
        "    tmax = 30. - 1. / raw.info['sfreq']\n",
        "    picks = mne.pick_types(raw.info, eeg=True, eog=True)\n",
        "    epochs = mne.Epochs(raw=raw, events=events, picks=picks, preload=True,\n",
        "                        event_id=event_id, tmin=0., tmax=tmax, baseline=None)\n",
        "\n",
        "    return epochs.get_data(), epochs.events[:, 2] - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6XeiaUTjr_l"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, ConcatDataset\n",
        "\n",
        "\n",
        "class EpochsDataset(Dataset):\n",
        "    \"\"\"Class to expose an MNE Epochs object as PyTorch dataset.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    epochs_data : np.ndarray\n",
        "        The epochs data, shape (n_epochs, n_channels, n_times).\n",
        "    epochs_labels : np.ndarray\n",
        "        The epochs labels, shape (n_epochs,)\n",
        "    subj_nb: None | int\n",
        "        Subject number.\n",
        "    rec_nb: None | int\n",
        "        Recording number.\n",
        "    transform : callable | None\n",
        "        The function to eventually apply to each epoch\n",
        "        for preprocessing (e.g. scaling). Defaults to None.\n",
        "    \"\"\"\n",
        "    def __init__(self, epochs_data, epochs_labels, subj_nb=None,\n",
        "                 rec_nb=None, transform=None):\n",
        "        assert len(epochs_data) == len(epochs_labels)\n",
        "        self.epochs_data = epochs_data\n",
        "        self.epochs_labels = epochs_labels\n",
        "        self.subj_nb = subj_nb\n",
        "        self.rec_nb = rec_nb\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.epochs_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X, y = self.epochs_data[idx], self.epochs_labels[idx]\n",
        "        if self.transform is not None:\n",
        "            X = self.transform(X)\n",
        "        X = torch.as_tensor(X[None, ...])\n",
        "        return X, y\n",
        "\n",
        "\n",
        "def scale(X):\n",
        "    \"\"\"Standard scaling of data along the last dimention.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array, shape (n_channels, n_times)\n",
        "        The input signals.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X_t : array, shape (n_channels, n_times)\n",
        "        The scaled signals.\n",
        "    \"\"\"\n",
        "    X -= np.mean(X, axis=1, keepdims=True)\n",
        "    return X / np.std(X, axis=1, keepdims=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4D2_vyIjr_l"
      },
      "source": [
        "We can now extract windows from each recording, and wrap them into Pytorch datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt73pxxJjr_l"
      },
      "outputs": [],
      "source": [
        "# Apply windowing and move to pytorch dataset\n",
        "all_datasets = [EpochsDataset(*extract_epochs(raw), subj_nb=raw.info['subject_info']['id'],\n",
        "                              rec_nb=raw.info['subject_info']['rec_id'], transform=scale)\n",
        "                for raw in raws]\n",
        "\n",
        "# Concatenate into a single dataset\n",
        "dataset = ConcatDataset(all_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCMMnMdTjr_l"
      },
      "source": [
        "You might have noticed we added a scaling transform to the datasets. This scaling makes sure each EEG channel in each 30-s window has a mean of 0 and a standard deviation of 1. This will help the neural network when training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeDve3X0jr_l"
      },
      "source": [
        "## 3. Making train, valid and test splits\n",
        "\n",
        "Now that we have our preprocessed and windowed data, we can split it into the different sets that we will need: (1) the **training set** is used to learn the parameters of our ConvNet, (2) the **validation set** is used to monitor the training process and decide when to stop it, and (3) the **test set** is used to provide an estimate of the generalization performance of our model.\n",
        "\n",
        "Here, we keep recording 1 of subjects 0-9 for testing, and split the remaining recordings into training and validation sets.\n",
        "\n",
        "We define the following functions to perform the split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yx69ydjhjr_l"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import LeavePGroupsOut\n",
        "\n",
        "\n",
        "def pick_recordings(dataset, subj_rec_nbs):\n",
        "    \"\"\"Pick recordings using subject and recording numbers.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset : ConcatDataset\n",
        "        The dataset to pick recordings from.\n",
        "    subj_rec_nbs : list of tuples\n",
        "        List of pairs (subj_nb, rec_nb) to use in split.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ConcatDataset\n",
        "        The picked recordings.\n",
        "    ConcatDataset | None\n",
        "        The remaining recordings. None if all recordings from\n",
        "        `dataset` were picked.\n",
        "    \"\"\"\n",
        "    pick_idx = list()\n",
        "    for subj_nb, rec_nb in subj_rec_nbs:\n",
        "        for i, ds in enumerate(dataset.datasets):\n",
        "            if (ds.subj_nb == subj_nb) and (ds.rec_nb == rec_nb):\n",
        "                pick_idx.append(i)\n",
        "\n",
        "    remaining_idx = np.setdiff1d(\n",
        "        range(len(dataset.datasets)), pick_idx)\n",
        "\n",
        "    pick_ds = ConcatDataset([dataset.datasets[i] for i in pick_idx])\n",
        "    if len(remaining_idx) > 0:\n",
        "        remaining_ds = ConcatDataset(\n",
        "            [dataset.datasets[i] for i in remaining_idx])\n",
        "    else:\n",
        "        remaining_ds = None\n",
        "\n",
        "    return pick_ds, remaining_ds\n",
        "\n",
        "\n",
        "def train_test_split(dataset, n_groups, split_by='subj_nb'):\n",
        "    \"\"\"Split dataset into train and test keeping n_groups out in test.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset : ConcatDataset\n",
        "        The dataset to split.\n",
        "    n_groups : int\n",
        "        The number of groups to leave out.\n",
        "    split_by : 'subj_nb' | 'rec_nb'\n",
        "        Property to use to split dataset.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ConcatDataset\n",
        "        The training data.\n",
        "    ConcatDataset\n",
        "        The testing data.\n",
        "    \"\"\"\n",
        "    groups = [getattr(ds, split_by) for ds in dataset.datasets]\n",
        "    train_idx, test_idx = next(\n",
        "        LeavePGroupsOut(n_groups).split(X=groups, groups=groups))\n",
        "\n",
        "    train_ds = ConcatDataset([dataset.datasets[i] for i in train_idx])\n",
        "    test_ds = ConcatDataset([dataset.datasets[i] for i in test_idx])\n",
        "\n",
        "    return train_ds, test_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmvyE7lkjr_m"
      },
      "outputs": [],
      "source": [
        "# We seed the random number generators to make our splits reproducible\n",
        "torch.manual_seed(87)\n",
        "np.random.seed(87)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsPPYf6Njr_m"
      },
      "outputs": [],
      "source": [
        "# Use recording 1 of subjects 0-9 as test set\n",
        "test_recs = [(subj_nb, rec_nb)  # DO NOT CHANGE! This is a fixed set.\n",
        "             for subj_nb, rec_nb in zip(range(10), [1] * 10)]\n",
        "test_ds, train_ds = pick_recordings(dataset, test_recs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XV35kD4Njr_m"
      },
      "outputs": [],
      "source": [
        "# Split remaining recordings into training and validation sets\n",
        "n_subjects_valid = max(1, int(len(train_ds.datasets) * 0.2))\n",
        "train_ds, valid_ds = train_test_split(train_ds, n_subjects_valid, split_by='subj_nb')\n",
        "\n",
        "print('Number of examples in each set:')\n",
        "print(f'Training: {len(train_ds)}')\n",
        "print(f'Validation: {len(valid_ds)}')\n",
        "print(f'Test: {len(test_ds)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myMN4J6Djr_m"
      },
      "source": [
        "Finally, we notice that the classes are imbalanced, i.e., there are a lot more of some classes than others. N2, especially, is more prevalent than the other sleep stages during the night:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMhpPCGMjr_m"
      },
      "outputs": [],
      "source": [
        "classes_mapping = {0: 'W', 1: 'N1', 2: 'N2', 3: 'N3', 4: 'R'}\n",
        "y_train = pd.Series([y for _, y in train_ds]).map(classes_mapping)\n",
        "ax = y_train.value_counts().plot(kind='barh')\n",
        "ax.set_xlabel('Number of training examples');\n",
        "ax.set_ylabel('Sleep stage');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h02HI-RTjr_m"
      },
      "source": [
        "One way to account for this imbalance during training is to give more weight to examples from rarer classes when computing the loss. We compute the weights with the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jblzmHHDjr_m"
      },
      "outputs": [],
      "source": [
        "# Computing class weight\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "train_y = np.concatenate([ds.epochs_labels for ds in train_ds.datasets])\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_y), y=train_y)\n",
        "print(class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMicy6xyjr_m"
      },
      "source": [
        "## 4. Creating the neural network\n",
        "\n",
        "In this section, we will define our ConvNet architecture.\n",
        "\n",
        "By default, we use the sleep staging architecture of Chambon et al. (2018), which looks something like this (adapted from Banville et al. 2020):\n",
        "\n",
        "![convnet](figs/convnet.png \"SleepStagerChambon2018\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KijxGZvvjr_n"
      },
      "source": [
        "The **input**, on the left, is a 30-s window of `C` channels. By default we have set `C` to 2 by selecting the 2 available EEG channels in Sleep Physionet above.\n",
        "\n",
        "The **output**, on the right, is a 5-dimensional vector where each dimension is matched to one of our 5 classes (W, N1, N2, N3 and R sleep stages).\n",
        "\n",
        "In between, we have a succession of convolutional layers, max pooling, and nonlinearities. The feature maps are finally flattened and passed through a fully-connected layer.\n",
        "\n",
        "We define the neural network in the following `torch.nn.Module` class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fWcillwjr_n"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "class SleepStagerChambon2018(nn.Module):\n",
        "    \"\"\"Sleep staging architecture from [1]_.\n",
        "\n",
        "    Convolutional neural network for sleep staging described in [1]_.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_channels : int\n",
        "        Number of EEG channels.\n",
        "    sfreq : float\n",
        "        EEG sampling frequency.\n",
        "    n_conv_chs : int\n",
        "        Number of convolutional channels. Set to 8 in [1]_.\n",
        "    time_conv_size_s : float\n",
        "        Size of filters in temporal convolution layers, in seconds. Set to 0.5\n",
        "        in [1]_ (64 samples at sfreq=128).\n",
        "    max_pool_size_s : float\n",
        "        Max pooling size, in seconds. Set to 0.125 in [1]_ (16 samples at\n",
        "        sfreq=128).\n",
        "    n_classes : int\n",
        "        Number of classes.\n",
        "    input_size_s : float\n",
        "        Size of the input, in seconds.\n",
        "    dropout : float\n",
        "        Dropout rate before the output dense layer.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] Chambon, S., Galtier, M. N., Arnal, P. J., Wainrib, G., &\n",
        "           Gramfort, A. (2018). A deep learning architecture for temporal sleep\n",
        "           stage classification using multivariate and multimodal time series.\n",
        "           IEEE Transactions on Neural Systems and Rehabilitation Engineering,\n",
        "           26(4), 758-769.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_channels, sfreq, n_conv_chs=8, time_conv_size_s=0.5,\n",
        "                 max_pool_size_s=0.125, n_classes=5, input_size_s=30,\n",
        "                 dropout=0.25):\n",
        "        super().__init__()\n",
        "\n",
        "        time_conv_size = int(time_conv_size_s * sfreq)\n",
        "        max_pool_size = int(max_pool_size_s * sfreq)\n",
        "        input_size = int(input_size_s * sfreq)\n",
        "        pad_size = time_conv_size // 2\n",
        "        self.n_channels = n_channels\n",
        "        len_last_layer = self._len_last_layer(\n",
        "            n_channels, input_size, max_pool_size, n_conv_chs)\n",
        "\n",
        "        if n_channels > 1:\n",
        "            self.spatial_conv = nn.Conv2d(1, n_channels, (n_channels, 1))\n",
        "\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                1, n_conv_chs, (1, time_conv_size), padding=(0, pad_size)),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((1, max_pool_size)),\n",
        "            nn.Conv2d(\n",
        "                n_conv_chs, n_conv_chs, (1, time_conv_size),\n",
        "                padding=(0, pad_size)),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((1, max_pool_size))\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(len_last_layer, n_classes)\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _len_last_layer(n_channels, input_size, max_pool_size, n_conv_chs):\n",
        "        return n_channels * (input_size // (max_pool_size ** 2)) * n_conv_chs\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Parameters\n",
        "        ---------\n",
        "        x: torch.Tensor\n",
        "            Batch of EEG windows of shape (batch_size, n_channels, n_times).\n",
        "        \"\"\"\n",
        "        if self.n_channels > 1:\n",
        "            x = self.spatial_conv(x)\n",
        "            x = x.transpose(1, 2)\n",
        "\n",
        "        x = self.feature_extractor(x)\n",
        "        return self.fc(x.flatten(start_dim=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTbQaX9Hjr_n"
      },
      "source": [
        "We instantiate our ConvNet with the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgYk63Lijr_n"
      },
      "outputs": [],
      "source": [
        "sfreq = raws[0].info['sfreq']  # Sampling frequency\n",
        "n_channels = raws[0].info['nchan']  # Number of channels\n",
        "\n",
        "model = SleepStagerChambon2018(n_channels, sfreq, n_classes=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBGFNKeDjr_n"
      },
      "source": [
        "Depending on whether a CUDA-enabled GPU is available, we can move the model to the GPU and perform the training there. This can enable significant speed-ups, but is not strictly required for this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8055twlpjr_n"
      },
      "outputs": [],
      "source": [
        "print(f'Using device \\'{device}\\'.')\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2OtYi38jr_r"
      },
      "source": [
        "## 5. Train and monitor network\n",
        "\n",
        "We are almost ready to finally train our ConvNet!\n",
        "\n",
        "We first need to define `DataLoader`s. `DataLoader` is a pytorch object that wraps a dataset and makes it easy to obtain batches of examples to feed to our neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjMuFAJdjr_r"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create dataloaders\n",
        "train_batch_size = 128  # Important hyperparameter\n",
        "valid_batch_size = 256  # Can be made as large as what fits in memory; won't impact performance\n",
        "num_workers = 0  # Number of processes to use for the data loading process; 0 is the main Python process\n",
        "\n",
        "loader_train = DataLoader(\n",
        "    train_ds, batch_size=train_batch_size, shuffle=True, num_workers=num_workers)\n",
        "loader_valid = DataLoader(\n",
        "    valid_ds, batch_size=valid_batch_size, shuffle=False, num_workers=num_workers)\n",
        "loader_test = DataLoader(\n",
        "    test_ds, batch_size=valid_batch_size, shuffle=False, num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NSFwazsjr_s"
      },
      "source": [
        "Next, we define a few functions to carry out our training and validation loops:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9dHypazjr_s"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import balanced_accuracy_score, cohen_kappa_score\n",
        "\n",
        "def _do_train(model, loader, optimizer, criterion, device, metric):\n",
        "    # training loop\n",
        "    model.train()\n",
        "\n",
        "    train_loss = np.zeros(len(loader))\n",
        "    y_pred_all, y_true_all = list(), list()\n",
        "    for idx_batch, (batch_x, batch_y) in enumerate(loader):\n",
        "        optimizer.zero_grad()\n",
        "        batch_x = batch_x.to(device=device, dtype=torch.float32)\n",
        "        batch_y = batch_y.to(device=device, dtype=torch.int64)\n",
        "\n",
        "        output = model(batch_x)\n",
        "        loss = criterion(output, batch_y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        y_pred_all.append(torch.argmax(output, axis=1).cpu().numpy())\n",
        "        y_true_all.append(batch_y.cpu().numpy())\n",
        "\n",
        "        train_loss[idx_batch] = loss.item()\n",
        "\n",
        "    y_pred = np.concatenate(y_pred_all)\n",
        "    y_true = np.concatenate(y_true_all)\n",
        "    perf = metric(y_true, y_pred)\n",
        "\n",
        "    return np.mean(train_loss), perf\n",
        "\n",
        "\n",
        "def _validate(model, loader, criterion, device, metric):\n",
        "    # validation loop\n",
        "    model.eval()\n",
        "\n",
        "    val_loss = np.zeros(len(loader))\n",
        "    y_pred_all, y_true_all = list(), list()\n",
        "    with torch.no_grad():\n",
        "        for idx_batch, (batch_x, batch_y) in enumerate(loader):\n",
        "            batch_x = batch_x.to(device=device, dtype=torch.float32)\n",
        "            batch_y = batch_y.to(device=device, dtype=torch.int64)\n",
        "            output = model.forward(batch_x)\n",
        "\n",
        "            loss = criterion(output, batch_y)\n",
        "            val_loss[idx_batch] = loss.item()\n",
        "\n",
        "            y_pred_all.append(torch.argmax(output, axis=1).cpu().numpy())\n",
        "            y_true_all.append(batch_y.cpu().numpy())\n",
        "\n",
        "    y_pred = np.concatenate(y_pred_all)\n",
        "    y_true = np.concatenate(y_true_all)\n",
        "    perf = metric(y_true, y_pred)\n",
        "\n",
        "    return np.mean(val_loss), perf\n",
        "\n",
        "\n",
        "def train(model, loader_train, loader_valid, optimizer, criterion, n_epochs,\n",
        "          patience, device, metric=None):\n",
        "    \"\"\"Training function.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : instance of nn.Module\n",
        "        The model.\n",
        "    loader_train : instance of Sampler\n",
        "        The generator of EEG samples the model has to train on.\n",
        "        It contains n_train samples\n",
        "    loader_valid : instance of Sampler\n",
        "        The generator of EEG samples the model has to validate on.\n",
        "        It contains n_val samples. The validation samples are used to\n",
        "        monitor the training process and to perform early stopping\n",
        "    optimizer : instance of optimizer\n",
        "        The optimizer to use for training.\n",
        "    n_epochs : int\n",
        "        The maximum of epochs to run.\n",
        "    patience : int\n",
        "        The patience parameter, i.e. how long to wait for the\n",
        "        validation error to go down.\n",
        "    metric : None | callable\n",
        "        Metric to use to evaluate performance on the training and\n",
        "        validation sets. Defaults to balanced accuracy.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    best_model : instance of nn.Module\n",
        "        The model that led to the best prediction on the validation\n",
        "        dataset.\n",
        "    history : list of dicts\n",
        "        Training history (loss, accuracy, etc.)\n",
        "    \"\"\"\n",
        "    best_valid_loss = np.inf\n",
        "    best_model = copy.deepcopy(model)\n",
        "    waiting = 0\n",
        "    history = list()\n",
        "\n",
        "    if metric is None:\n",
        "        metric = balanced_accuracy_score\n",
        "\n",
        "    print('epoch \\t train_loss \\t valid_loss \\t train_perf \\t valid_perf')\n",
        "    print('-------------------------------------------------------------------')\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        train_loss, train_perf = _do_train(\n",
        "            model, loader_train, optimizer, criterion, device, metric=metric)\n",
        "        valid_loss, valid_perf = _validate(\n",
        "            model, loader_valid, criterion, device, metric=metric)\n",
        "        history.append(\n",
        "            {'epoch': epoch,\n",
        "             'train_loss': train_loss, 'valid_loss': valid_loss,\n",
        "             'train_perf': train_perf, 'valid_perf': valid_perf})\n",
        "\n",
        "        print(f'{epoch} \\t {train_loss:0.4f} \\t {valid_loss:0.4f} '\n",
        "              f'\\t {train_perf:0.4f} \\t {valid_perf:0.4f}')\n",
        "\n",
        "        # model saving\n",
        "        if valid_loss < best_valid_loss:\n",
        "            print(f'best val loss {best_valid_loss:.4f} -> {valid_loss:.4f}')\n",
        "            best_valid_loss = valid_loss\n",
        "            best_model = copy.deepcopy(model)\n",
        "            waiting = 0\n",
        "        else:\n",
        "            waiting += 1\n",
        "\n",
        "        # model early stopping\n",
        "        if waiting >= patience:\n",
        "            print(f'Stop training at epoch {epoch}')\n",
        "            print(f'Best val loss : {best_valid_loss:.4f}')\n",
        "            break\n",
        "\n",
        "    return best_model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nBqA8vKjr_s"
      },
      "source": [
        "Two critical pieces of the training process are the **optimizer** and the **criterion**.\n",
        "\n",
        "* The **optimizer** implements the parameter update procedure. Here, we use `Adam`, a popular adaptive gradient descent optimizer for deep neural networks.\n",
        "* The **criterion**, or loss function, is used to measure how well the neural network performs on an example. Here, we use the standard multiclass cross-entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nftHCU9Qjr_s"
      },
      "outputs": [],
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=0)\n",
        "criterion = CrossEntropyLoss(weight=torch.Tensor(class_weights).to(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFDCZ9yZjr_s"
      },
      "source": [
        "We can now launch our training loop. The maxmium number of training epochs (or \"passes\" through the training set) is set with `n_epochs`. The `patience` hyperparameter controls how many epochs we will wait for before stopping the training process if there is no improvement on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhG1MaSPjr_s"
      },
      "outputs": [],
      "source": [
        "n_epochs = 10\n",
        "patience = 5\n",
        "\n",
        "best_model, history = train(\n",
        "    model, loader_train, loader_valid, optimizer, criterion, n_epochs, patience,\n",
        "    device, metric=cohen_kappa_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN02yE-Yjr_t"
      },
      "source": [
        "Next, we visualize the results of our training.\n",
        "\n",
        "First, the training curves show how the loss and accuracy improved across training epochs. We use [Cohen's kappa](https://scikit-learn.org/stable/modules/model_evaluation.html#cohen-kappa) (instead of the standard accuracy) to better reflect performance under class imbalance and allow comparison with results from the literature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSY1ROx_jr_t"
      },
      "outputs": [],
      "source": [
        "# Visualizing the learning curves\n",
        "\n",
        "history_df = pd.DataFrame(history)\n",
        "ax1 = history_df.plot(x='epoch', y=['train_loss', 'valid_loss'], marker='o')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax2 = history_df.plot(x='epoch', y=['train_perf', 'valid_perf'], marker='o')\n",
        "ax2.set_ylabel('Cohen\\'s kappa')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX3Hr3IQjr_t"
      },
      "source": [
        "We expect the loss to decrease and the Cohen's kappa to increase as more and more training epochs are performed.\n",
        "\n",
        "We also measure the performance on the test set, which was not seen during training. This gives us a better estimate of the generalization performance of our ConvNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ka3SXhR_jr_t"
      },
      "outputs": [],
      "source": [
        "# Compute test performance\n",
        "\n",
        "best_model.eval()\n",
        "\n",
        "y_pred_all, y_true_all = list(), list()\n",
        "for batch_x, batch_y in loader_test:\n",
        "    batch_x = batch_x.to(device=device, dtype=torch.float32)\n",
        "    batch_y = batch_y.to(device=device, dtype=torch.int64)\n",
        "    output = model.forward(batch_x)\n",
        "    y_pred_all.append(torch.argmax(output, axis=1).cpu().numpy())\n",
        "    y_true_all.append(batch_y.cpu().numpy())\n",
        "\n",
        "y_pred = np.concatenate(y_pred_all)\n",
        "y_true = np.concatenate(y_true_all)\n",
        "rec_ids = np.concatenate(  # indicates which recording each example comes from\n",
        "    [[i] * len(ds) for i, ds in enumerate(test_ds.datasets)])\n",
        "\n",
        "test_bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
        "test_kappa = cohen_kappa_score(y_true, y_pred)\n",
        "\n",
        "print(f'Test balanced accuracy: {test_bal_acc:0.3f}')\n",
        "print(f'Test Cohen\\'s kappa: {test_kappa:0.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9cyL_mUjr_t"
      },
      "source": [
        "For our 5-class problem, chance-level would correspond to 20% balanced accuracy or a Cohen's kappa of 0.0.\n",
        "\n",
        "To get a sense of what is possible, a recent model achieved a kappa of 0.814 on the Sleep Physionet data using a single EEG channel (10-fold cross-validation):\n",
        "\n",
        "> Phan, H., Chén, O. Y., Koch, P., Mertins, A., & De Vos, M. (2020). Xsleepnet: Multi-view sequential model for automatic sleep staging. arXiv preprint arXiv:2007.05492"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhZzkx-qjr_t"
      },
      "source": [
        "## 6. Visualizing results\n",
        "\n",
        "We further inspect the results in this section.\n",
        "\n",
        "We start by looking at the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix), which shows which classes were easier or more difficult to classify for our ConvNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWPdq91wjr_u"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion_matrix(conf_mat, classes_mapping):\n",
        "    ticks = list(classes_mapping.keys())\n",
        "    tick_labels = classes_mapping.values()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    im = ax.imshow(conf_mat, cmap='Reds')\n",
        "\n",
        "    ax.set_yticks(ticks)\n",
        "    ax.set_yticklabels(tick_labels)\n",
        "    ax.set_xticks(ticks)\n",
        "    ax.set_xticklabels(tick_labels)\n",
        "    ax.set_ylabel('True label')\n",
        "    ax.set_xlabel('Predicted label')\n",
        "    ax.set_title('Confusion matrix')\n",
        "\n",
        "    for i in range(len(ticks)):\n",
        "        for j in range(len(ticks)):\n",
        "            text = ax.text(\n",
        "                j, i, conf_mat[i, j], ha='center', va='center', color='k')\n",
        "\n",
        "    fig.colorbar(im, ax=ax, fraction=0.05, label='# examples')\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVXn_YYojr_u"
      },
      "outputs": [],
      "source": [
        "conf_mat = confusion_matrix(y_true, y_pred)\n",
        "plot_confusion_matrix(conf_mat, classes_mapping);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oGI7skAjr_u"
      },
      "source": [
        "What kind of mistakes does the ConvNet seem to make? Is there a class that's often mistaken for another one?\n",
        "\n",
        "We can also visualize the predictions on a recording basis. This visualization is known as a \"[hypnogram](https://en.wikipedia.org/wiki/Hypnogram)\". A hypnogram shows the evolution of sleep stages across an overnight recording."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6CkHzWBjr_u"
      },
      "outputs": [],
      "source": [
        "# Plot hypnogram for one recording\n",
        "\n",
        "mask = rec_ids == 0  # pick a recording number\n",
        "\n",
        "t = np.arange(len(y_true[mask])) * 30 / 3600\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 3))\n",
        "ax.plot(t, y_true[mask], label='True')\n",
        "ax.plot(t, y_pred[mask], alpha=0.7, label='Predicted')\n",
        "ax.set_yticks([0, 1, 2, 3, 4])\n",
        "ax.set_yticklabels(['W', 'N1', 'N2', 'N3', 'R'])\n",
        "ax.set_xlabel('Time (h)')\n",
        "ax.set_title('Hypnogram')\n",
        "ax.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HIUd6fKjr_u"
      },
      "source": [
        "Do the predictions of the ConvNet follow the groundtruth hypnogram? Is there any structure in the way mistakes are made?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVshPhdXjr_u"
      },
      "source": [
        "## Going further\n",
        "\n",
        "You've now covered all the material for this tutorial!\n",
        "\n",
        "To test what you have learned, we recommend you identify a few key elements in the pipeline shown above, and play with them to try to improve the performance of your ConvNet. Here are a few ideas to get you started:\n",
        "- Increasing the training set size\n",
        "- Improving the architecture (you can look at recent sleep staging literature, or follow your intuition!)\n",
        "- Optimizing the training hyperparameters (learning rate, batch size, etc.)\n",
        "\n",
        "Good luck! :)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}